{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 21 13:52:36 2023\n",
    "\n",
    "@author: Hamidreza (h.r.jahangir@gmail.com)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Input, Dropout, MaxPooling2D, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import xlsxwriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from keras import regularizers\n",
    "import sys\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#%% Define Inverse One-hot \n",
    "def inv_one_hot(OH_data):\n",
    "    max_index_col = np.argmax(OH_data[0])+1\n",
    "    out = list ()\n",
    "    for i in range(0,OH_data.shape[0]):\n",
    "          max_index_col = np.argmax(OH_data[i]) +1\n",
    "          out.append(max_index_col)\n",
    "          \n",
    "    out = np.array(out)\n",
    "    return out\n",
    "#%%\n",
    "\n",
    "\n",
    "# Defining a general class for designing a specific network for all cases\n",
    "# Note that the Deep_Net_Structure class is the main class and other classes are defined as subclasses of this class\n",
    "\n",
    "class Deep_Net_Structure:\n",
    "    \n",
    "    # Initializing the parameters for Deep_Net_Structure class\n",
    "    \n",
    "    def __init__(self, CNN_layer_list, Dense_layer_list, Decoder_layer_list, input_shape, output_size, kernel_size, pool_size, stride_size):\n",
    "        self.CNN_layer_list = np.array(CNN_layer_list)\n",
    "        self.Dense_layer_list = np.array(Dense_layer_list)\n",
    "        self.Decoder_layer_list = np.array(Decoder_layer_list)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = np.array(output_size)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.stride_size = stride_size\n",
    "    \n",
    "    # Defining a function to create a deep neural network\n",
    "    \n",
    "    def Creat_Network(self):\n",
    "        \n",
    "         # size of input data is  (number of generators) *  (number of samples) * (number of features or channels) \n",
    "        input_shape = Input(shape=self.input_shape)     \n",
    "        # First Convolutional Layer\n",
    "        conv1 = Conv2D( self.CNN_layer_list[0], self.kernel_size[0], strides = self.stride_size[0], activation = 'relu', padding = 'valid')(input_shape)\n",
    "        # Adding the pooling layer for the dimension reduction\n",
    "        pool1 = MaxPooling2D (pool_size =self.pool_size[0], strides =self.stride_size[1],padding= \"valid\")(conv1)\n",
    "        # Adding the Dropout layer for improving the training against the overfitting\n",
    "        dropout1= Dropout(0.1)(pool1)     \n",
    "    \n",
    "        # Second Convolutional Layer\n",
    "        conv2 = Conv2D(self.CNN_layer_list[1], self.kernel_size[1], strides = self.stride_size[2], padding = 'same')(dropout1)\n",
    "        # Adding the pooling layer for dimension reduction\n",
    "        pool2 = MaxPooling2D (pool_size =self.pool_size[1], strides =self.stride_size[3], padding= \"valid\")(conv2)\n",
    "        # Adding the Dropout layer for improving the training against the overfitting\n",
    "        dropout2= Dropout(0.1)(pool2) \n",
    "        # Flatten data\n",
    "        flat = Flatten()(dropout2)\n",
    "        \n",
    "        # Classification Output Layer (Localization task)\n",
    "        hidden1 = Dense(self.Dense_layer_list[0],activation = 'relu')(flat)\n",
    "        output = Dense (self.output_size, activation = 'softmax')(hidden1)\n",
    "        \n",
    "        # Reconstruction Network (using AE framework with MLP structure)\n",
    "        decoder1 = Dense(self.Decoder_layer_list[0], activation = 'relu', activity_regularizer=regularizers.l1(10e-5))(flat)\n",
    "        decoder2 = Dense(self.Decoder_layer_list[1], activation = 'relu')(decoder1)\n",
    "        decoder3 = Dense(self.Decoder_layer_list[2], activation = 'relu')(decoder2)\n",
    "        decoderoutput = Reshape(self.input_shape)(decoder3)\n",
    "        \n",
    "        # Defining the multi-output model (Localization layer and Reconstruction layer)\n",
    "        model = Model(inputs=input_shape, outputs=[output, decoderoutput])\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "# Defining the IEEE cases including the IEEE 14-bus system, IEEE 39-bus system, and IEEE 57-bus system\n",
    "# Note that the following is a subclass of the Deep_Net_Structure class\n",
    "\n",
    "class IEEE_14_bus_model(Deep_Net_Structure):\n",
    "\n",
    "    def __init__(self, CNN_layer_list , Dense_layer_list):\n",
    "                                                        # Decoder_layers   ,  input,  output, kernel_size,  pool_size,   stride_size\n",
    "        super().__init__(CNN_layer_list,Dense_layer_list,[256, 512, 5*100*2], (5,100,2),9, ((1,20),(2,2)),((2,2),(2,2)),((1,20),(1,1),(1,1),(1,1)))    \n",
    "\n",
    "class IEEE_39_bus_model(Deep_Net_Structure):\n",
    "\n",
    "    def __init__(self, CNN_layer_list , Dense_layer_list):\n",
    "                                                        # Decoder_layers   ,  input,  output, kernel_size,  pool_size,   stride_size\n",
    "        super().__init__(CNN_layer_list,Dense_layer_list,[512, 1024, 10*100*2], (10,100,2),29,((1,10),(2,2)), ((2,2),(2,2)), ((1,10),(2,2),(1,1),(2,2)))\n",
    "\n",
    "class IEEE_57_bus_model(Deep_Net_Structure):\n",
    "\n",
    "    def __init__(self, CNN_layer_list , Dense_layer_list):\n",
    "                                                        # Decoder_layers   ,  input,  output, kernel_size,  pool_size,   stride_size\n",
    "        super().__init__(CNN_layer_list,Dense_layer_list,[512, 1024, 7*100*2], (7,100,2),50,((1,10),(2,2)), ((2,5),(2,2)), ((1,10),(1,2),(1,1),(2,2)))\n",
    "\n",
    "\n",
    "#%% \n",
    "\n",
    "# Defining a Switch class for subclasses of the Deep_Net_Structure \n",
    "\n",
    "class SwitchCase:\n",
    "\n",
    "\n",
    "    # Initialization of the switch class\n",
    "    \n",
    "    def switch(self, Bus_Number):\n",
    "        default = \"Invalid Case\"\n",
    "        return getattr(self, 'IEEE_case_' + str(Bus_Number) + '_Bus', lambda: default)()\n",
    "\n",
    "    # Defining the parameters of the IEEE 14-bus switch class\n",
    "    \n",
    "    def IEEE_case_14_Bus(self):\n",
    "        \n",
    "        # Reading the input data from the .mat files\n",
    "\n",
    "        profiles = spio.loadmat('profiles_IEEE_14_Bus_Lim.mat', squeeze_me=True)\n",
    "        labels = spio.loadmat('labels_IEEE_14_Bus_Lim.mat', squeeze_me=True)\n",
    "        profiles = profiles [\"profiles\"]\n",
    "        labels = labels [\"labels\"]\n",
    "        model_name = 'IEEE_case_14_Bus'\n",
    "        \n",
    "        # Defining the training parameters of this case\n",
    "        \n",
    "        reconstruction_training_loss_parameter = 0.0005\n",
    "        patience_rate = 2\n",
    "        learning_rate_value = 0.00005\n",
    "        batch_size = 10\n",
    "        \n",
    "        # Creating the network for this case using the subclass IEEE 14-bus case from Deep_Net_Structure class\n",
    "        \n",
    "        model = IEEE_14_bus_model([512,256,128], [128]).Creat_Network()\n",
    "        return profiles, labels, model,model_name, reconstruction_training_loss_parameter, patience_rate, learning_rate_value, batch_size\n",
    "\n",
    "    def IEEE_case_39_Bus(self):\n",
    "        \n",
    "        # Reading the input data from the .mat files\n",
    "        \n",
    "        profiles = spio.loadmat('profiles_IEEE_39_Bus_lim.mat', squeeze_me=True)\n",
    "        labels = spio.loadmat('labels_IEEE_39_Bus_lim.mat', squeeze_me=True)\n",
    "        profiles = profiles [\"profiles\"]\n",
    "        labels = labels [\"labels\"]\n",
    "        model_name = 'IEEE_case_39_Bus'\n",
    "        \n",
    "        # Defining the training parameters of this case\n",
    "        \n",
    "        reconstruction_training_loss_parameter = 0.0005\n",
    "        patience_rate = 30\n",
    "        learning_rate_value = 0.00005\n",
    "        batch_size = 20\n",
    "        \n",
    "        # Creating the network for this case using the subclass IEEE 39-bus case from Deep_Net_Structure class\n",
    "        \n",
    "        model = IEEE_39_bus_model([512,256,128], [128]).Creat_Network()\n",
    "        return profiles, labels, model,model_name, reconstruction_training_loss_parameter, patience_rate,learning_rate_value, batch_size\n",
    "\n",
    "    def IEEE_case_57_Bus(self):\n",
    "        \n",
    "        # Reading the input data from the .mat files\n",
    "        \n",
    "        profiles = spio.loadmat('profiles_IEEE_57_Bus_lim.mat', squeeze_me=True)\n",
    "        labels = spio.loadmat('labels_IEEE_57_Bus_lim.mat', squeeze_me=True)\n",
    "        profiles = profiles [\"profiles\"]\n",
    "        labels = labels [\"labels\"]\n",
    "        model_name = 'IEEE_case_57_Bus'\n",
    "        \n",
    "        # Defining the training parameters of this case\n",
    "        \n",
    "        reconstruction_training_loss_parameter = 0.0005\n",
    "        patience_rate = 10\n",
    "        learning_rate_value = 0.00005\n",
    "        batch_size = 20\n",
    "        \n",
    "        # Creating the network for this case using the subclass IEEE 57-bus case from Deep_Net_Structure class\n",
    "\n",
    "        model = IEEE_57_bus_model([512,256,128], [128]).Creat_Network()\n",
    "        return profiles, labels, model,model_name, reconstruction_training_loss_parameter, patience_rate, learning_rate_value, batch_size\n",
    "\n",
    "\n",
    "# running the switch class\n",
    "case = SwitchCase()\n",
    "\n",
    "# selecting the IEEE case class\n",
    "\n",
    "print('Enter the total bus number of the IEEE case (14, 39, or 57):')\n",
    "IEEE_case_bus_number = input()\n",
    "\n",
    "Structure = case.switch(IEEE_case_bus_number)\n",
    "\n",
    "if Structure == 'Invalid Case':\n",
    "    sys.exit(\"Invalid Case, This program is designed for IEEE 14 Bus, IEEE 39 Bus, and IEEE 57 Bus cases\")\n",
    "\n",
    "# Initializing the parameters based on the selected class\n",
    "# Note that for understanding the parameters of the selected class, please refer to the subfunctions in the switch class\n",
    "\n",
    "profiles = Structure[0]\n",
    "labels = Structure[1]\n",
    "model_name = Structure[3]\n",
    "reconstruction_loss_parameter = Structure[4]\n",
    "patience_rate = Structure[5]\n",
    "learning_rate_value = Structure[6]\n",
    "batch_size_parameter = Structure[7]\n",
    "     \n",
    "\n",
    "#%%\n",
    "\n",
    "X_total = profiles         # Input data (delta and omega of generator buses)\n",
    "y = labels                 # Classes (number of attacked buses [load buses])\n",
    "sample_window = 100        # Number of monitored samples per each observation\n",
    "\n",
    "X = X_total[:,:,0:sample_window,:]\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32').reshape(y.shape[0],1)\n",
    "\n",
    "y = np.array(to_categorical(y.astype('float32')))  # define a categorical format for each label (One-hot)\n",
    "y = np.delete(y,0,1)                               # remove the first column (zero class ) (the extra column is added due to the categorical procedure)\n",
    "training_percentage = 0.8;                         # dividing data set into training and test parts\n",
    "validation_percentage = 0.1;                       # dividing data set into training and test parts\n",
    "divide_data1 = int(training_percentage*profiles.shape[0])                          # define a dividing parameter\n",
    "divide_data2 = int((validation_percentage+training_percentage)*profiles.shape[0])  # define a dividing parameter\n",
    "\n",
    "# Defining training and testing parts\n",
    "\n",
    "x_train = X [0:divide_data1,:]\n",
    "x_valid = X [0:divide_data2,:]\n",
    "x_test = X [divide_data2:len(profiles),:]\n",
    "y_train = y [0:divide_data1,:]\n",
    "y_valid = y [0:divide_data2,:]\n",
    "y_test = y [divide_data2:len(profiles),:]\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "epochs = 1500   # Defining the maximum number of epochs\n",
    "\n",
    "# Defining early stopping criteria\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience= patience_rate )\n",
    "mc = ModelCheckpoint(str(model_name)+ '_CNN.h5', monitor='val_loss', mode='min', verbose=1, save_best_only = 'True')\n",
    "#save_freq = 'epoch'\n",
    "\n",
    "\n",
    "# Defining the operation mode (we have two modes: training and testing)\n",
    "\n",
    "print('Please select the operation mode (enter \"test\" for testing and enter \"train\" for training):')\n",
    "operation_mode = input()\n",
    "\n",
    "\n",
    "if operation_mode == 'train':\n",
    "    model =  Structure[2]\n",
    "    print('Training')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_value),loss= ['categorical_crossentropy','mse'] ,loss_weights = [1., reconstruction_loss_parameter],metrics=['acc'])\n",
    "    history = model.fit([x_train],[y_train, x_train], batch_size = batch_size_parameter, epochs = epochs, validation_data = ([x_valid],[y_valid, x_valid]),callbacks=[es, mc])\n",
    "    \n",
    "    # plot training based on the accuracy\n",
    "\n",
    "    plt.plot(history.history[list(history.history.keys())[3]], label='train')\n",
    "    plt.plot(history.history[list(history.history.keys())[8]], label='validation')\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # plot training based on the loss\n",
    "\n",
    "    plt.plot(history.history[list(history.history.keys())[0]])\n",
    "    plt.plot(history.history[list(history.history.keys())[5]])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    label_predicted_test = model.predict(x_test)\n",
    "    label_predicted_train = model.predict(x_train)\n",
    "    label_predicted_test = model.predict(x_test)\n",
    "    \n",
    "    # Apply the defined inverse one-hot function for calculating the accuracy\n",
    "    \n",
    "    final_accuracy_test = accuracy_score(inv_one_hot(y_test), inv_one_hot(label_predicted_test[0]))\n",
    "    \n",
    "    print(final_accuracy_test)\n",
    "    #%%\n",
    "\n",
    "    #Train Acc\n",
    "    train_acc = np.array(history.history[list(history.history.keys())[3]])\n",
    "\n",
    "    #Valid Acc\n",
    "\n",
    "    valid_acc = np.array(history.history[list(history.history.keys())[8]])\n",
    "\n",
    "    #Train Loss\n",
    "    train_loss = np.array(history.history[list(history.history.keys())[0]])\n",
    "\n",
    "    #Valid Loss\n",
    "    valid_loss = np.array(history.history[list(history.history.keys())[5]])\n",
    "\n",
    "    # Preparation for writing the results into the Excel file\n",
    "    \n",
    "    total_acc = np.array([train_acc,valid_acc])\n",
    "    total_acc = np.transpose(total_acc)\n",
    "\n",
    "    total_loss = np.array([train_loss,valid_loss])\n",
    "    total_loss = np.transpose(total_loss)\n",
    "\n",
    "    total_train_labels = np.array([inv_one_hot(y_train),inv_one_hot(label_predicted_train[0])])\n",
    "    total_train_labels = np.transpose(total_train_labels)\n",
    "\n",
    "    total_test_labels = np.array([inv_one_hot(y_test),inv_one_hot(label_predicted_test[0])])\n",
    "    total_test_labels = np.transpose(total_test_labels)\n",
    "\n",
    "    # Write the results into the Excel file \n",
    "    workbook = xlsxwriter.Workbook(str(model_name)+'_CNN.h5'+'.xlsx')\n",
    "    worksheet = workbook.add_worksheet('Accuracy')   # training & validation\n",
    "\n",
    "    #Write column names\n",
    "    worksheet.write(0, 0, \"Traning\")\n",
    "    worksheet.write(0, 1, \"Validation\")\n",
    "\n",
    "    row = 1\n",
    "    for col, data in enumerate(np.transpose(total_acc)):\n",
    "        worksheet.write_column(row, col, data)\n",
    "\n",
    "    worksheet = workbook.add_worksheet('Loss')     # training & validation\n",
    "\n",
    "    #Write column names\n",
    "    worksheet.write(0, 0, \"Traning\")\n",
    "    worksheet.write(0, 1, \"Validation\")\n",
    "\n",
    "    row = 1\n",
    "    for col, data in enumerate(np.transpose(total_loss)):\n",
    "        worksheet.write_column(row, col, data)\n",
    "        \n",
    "        \n",
    "    worksheet = workbook.add_worksheet('Training labels')     # training & validation\n",
    "\n",
    "    #write column names\n",
    "    worksheet.write(0, 0, \"Actual\")\n",
    "    worksheet.write(0, 1, \"Estimated\")\n",
    "\n",
    "    row = 1\n",
    "    for col, data in enumerate(np.transpose(total_train_labels)):\n",
    "        worksheet.write_column(row, col, data)\n",
    "        \n",
    "\n",
    "\n",
    "    worksheet = workbook.add_worksheet('Test labels')     # training & validation\n",
    "\n",
    "    #Write column names\n",
    "    worksheet.write(0, 0, \"Actual\")\n",
    "    worksheet.write(0, 1, \"Estimated\")\n",
    "\n",
    "    row = 1\n",
    "    for col, data in enumerate(np.transpose(total_test_labels)):\n",
    "        worksheet.write_column(row, col, data)\n",
    "        \n",
    "    workbook.close()\n",
    "    \n",
    "elif operation_mode == 'test':\n",
    "    model = load_model(str(model_name)+ '_CNN.h5')\n",
    "else:\n",
    "    sys.exit(\"Invalid Input data\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#%%\n",
    "def plot_confusion_matrix(true_labels, predicted_labels):\n",
    "    classes = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=classes)\n",
    "    \n",
    "    # Create a figure with higher resolution\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=100)\n",
    "    \n",
    "    # Create axes for the plot\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    # Plot the confusion matrix as an image\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set the colorbar label\n",
    "    cbar.ax.set_ylabel('Count', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    # Set the x and y axis labels\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    \n",
    "    # Set the tick labels\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    ax.set_xticklabels(classes, rotation=45)\n",
    "    ax.set_yticklabels(classes)\n",
    "    \n",
    "    # Set the threshold for text color in the cells\n",
    "    threshold = cm.max() / 2.0\n",
    "    \n",
    "    # Loop over the data dimensions and create text annotations\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > threshold else \"black\")\n",
    "    \n",
    "    # Set the title of the plot\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    \n",
    "    # Show the plot in a new window\n",
    "    plt.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "def plot_time_series(a,b):\n",
    "    \n",
    "    \n",
    "    profiles_set1 =a[:,0:100]*50+50\n",
    "    profiles_set2 =b[:,0:100]\n",
    "    # Determine the x-axis values (assuming equal spacing)\n",
    "    \n",
    "    x = np.arange(len(profiles_set1[0]))\n",
    "\n",
    "    # Create a figure with higher resolution\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=100)\n",
    "\n",
    "    # Create subplots for each set of profiles\n",
    "    ax1 = fig.add_subplot(2, 1, 1)\n",
    "    ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "    # Plot the profiles on separate subplots\n",
    "    for subplot in profiles_set1:\n",
    "        ax1.plot(x, subplot)\n",
    "\n",
    "    for subplot in profiles_set2:\n",
    "        ax2.plot(x, subplot)\n",
    "\n",
    "    # Set the y-axis limits based on the profiles\n",
    "    min_val1 = (np.min(profiles_set1))\n",
    "    max_val1 = (np.max(profiles_set1))\n",
    "    \n",
    "    min_val2 = (np.min(profiles_set2))\n",
    "    max_val2 = (np.max(profiles_set2))\n",
    "    \n",
    "    ax1.set_ylim(min_val1, max_val1)\n",
    "    ax2.set_ylim(min_val2, max_val2)\n",
    "\n",
    "    # Set the titles for each subplot\n",
    "    ax1.set_title('Frequency')\n",
    "    ax2.set_title('Voltage Phase Angle')\n",
    "\n",
    "    # Set the x-axis label\n",
    "    fig.text(0.5, 0.04, 'Time', ha='center')\n",
    "    \n",
    "    # Set the y-axis label for the first subplot\n",
    "    fig.text(0.0, 0.75, 'Hz', va='center', rotation='vertical', fontsize=12, color='black')\n",
    "    ax1.yaxis.set_label_coords(-0.08, 0.5)\n",
    "\n",
    "    # Set the y-axis label for the second subplot\n",
    "    fig.text(0.0, 0.25, 'Voltage Phase Angle', va='center', rotation='vertical', fontsize=12, color='black')\n",
    "    ax2.yaxis.set_label_coords(1.08, 0.5)\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Show the plot in a new window\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# Testing mode\n",
    "# Calculate the accuracy and time\n",
    "\n",
    "start_time = time.time()\n",
    "label_predicted_test = model.predict(x_test)\n",
    "print(\"--- needed time for the localization task is %s seconds ---\" % (time.time() - start_time))\n",
    "label_predicted_train = model.predict(x_train)\n",
    "label_predicted_test = model.predict(x_test)\n",
    "final_accuracy_test = accuracy_score(inv_one_hot(y_test), inv_one_hot(label_predicted_test[0]))\n",
    "print('localizaion accuracy is =', final_accuracy_test)\n",
    "\n",
    "#%%\n",
    "\n",
    "# Writing the test outcome into the Excel file\n",
    "\n",
    "total_test_labels = np.array([inv_one_hot(y_test),inv_one_hot(label_predicted_test[0])])\n",
    "total_test_labels = np.transpose(total_test_labels)\n",
    "\n",
    "workbook = xlsxwriter.Workbook(str(model_name)+'_CNN.h5_'+'test'+'.xlsx')\n",
    "worksheet = workbook.add_worksheet('Test labels')     # training & validation\n",
    "\n",
    "#write column names\n",
    "worksheet.write(0, 0, \"Actual\")\n",
    "worksheet.write(0, 1, \"Estimated\")\n",
    "\n",
    "row = 1\n",
    "for col, data in enumerate(np.transpose(total_test_labels)):\n",
    "    worksheet.write_column(row, col, data)\n",
    "    \n",
    "workbook.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(total_test_labels[:,0], total_test_labels[:,1])\n",
    "\n",
    "plot_time_series(np.reshape(profiles [9,:,:,0], (profiles.shape[1],profiles.shape[2])), np.reshape(profiles [9,:,:,1], (profiles.shape[1],profiles.shape[2])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
